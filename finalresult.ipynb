{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bản sao của finalresult.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPtGSQxl2v0q79OvfVliq+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntdthanh1409/Baitap/blob/master/finalresult.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBy0gvC5_LC0"
      },
      "source": [
        "import os\r\n",
        "import bs4\r\n",
        "import requests\r\n",
        "\r\n",
        "\r\n",
        "#  Mỗi website bạn cào là một thư mục mới\r\n",
        "def create_project_dir(directory):\r\n",
        "    if not os.path.exists(directory):\r\n",
        "        os.makedirs(directory)\r\n",
        "        print(\"Creating project: \" + directory)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#  Tạo Queue (hàng đợi) và crawled files (nếu chưa được tạo)\r\n",
        "def create_data_files(project_name, base_url):\r\n",
        "    queue = project_name + '/queue.txt'\r\n",
        "    crawled = project_name + '/crawled.txt'\r\n",
        "    if not os.path.isfile(queue):\r\n",
        "        write_file(queue, base_url)\r\n",
        "    if not os.path.isfile(crawled):\r\n",
        "        write_file(crawled, '')\r\n",
        "\r\n",
        "\r\n",
        "#  Tạo file mới\r\n",
        "def write_file(path, data):\r\n",
        "    f = open(path, 'a')\r\n",
        "    f.write(data)\r\n",
        "    f.close()\r\n",
        "\r\n",
        "\r\n",
        "#  Thêm dữ liệu vào file đã tồn tại\r\n",
        "def append_to_file(path, data):\r\n",
        "    with open(path, 'a') as file:\r\n",
        "        file.write(data + '\\n')\r\n",
        "\r\n",
        "\r\n",
        "#Xoá nội dung trong file\r\n",
        "def delete_file_contents(path):\r\n",
        "    with open(path, 'w'):\r\n",
        "        pass\r\n",
        "\r\n",
        "#  Đọc file và chuyển file vào set\r\n",
        "def file_to_set(file_name):\r\n",
        "    ket_qua = set()\r\n",
        "    with open(file_name, 'rt') as f:\r\n",
        "        for line in f:\r\n",
        "            ket_qua.add(line.replace('/n', ''))\r\n",
        "    return ket_qua\r\n",
        "\r\n",
        "\r\n",
        "#  lặp url qua set để không craw trùng, mỗi url sẽ là một dòng mới trong file\r\n",
        "def set_to_file(links, file):\r\n",
        "    delete_file_contents(file)\r\n",
        "    for link in sorted(links):\r\n",
        "        append_to_file(file, link)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS54mpjc_Qt6"
      },
      "source": [
        "def crawl(url, destination_set, final_set, max_results):\r\n",
        "  if url not in history:\r\n",
        "    history.append(url)\r\n",
        "    res = requests.get(url)\r\n",
        "    ressoup = bs4.BeautifulSoup(res.text, features=\"html.parser\")\r\n",
        "    for a in ressoup.find_all('a'):\r\n",
        "        link = a.get('href')\r\n",
        "        if link == None:\r\n",
        "          continue\r\n",
        "        else:\r\n",
        "          if link.startswith('/'):\r\n",
        "            link = url + link\r\n",
        "          if link.startswith(seed):\r\n",
        "            destination_set.add(link)\r\n",
        "            final_set.add(link)\r\n",
        "            if len(final_set) == max_results:\r\n",
        "              return False # Dừng khi đã đạt kết quả tối đa\r\n",
        "          else:\r\n",
        "            continue\r\n",
        "    return True # dừng sau khi duyệt hết"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7t1OcsELUlr"
      },
      "source": [
        "def crawl_set(parent_set, child_set, final_set, max_results):\r\n",
        "  for link in parent_set:\r\n",
        "    flag = crawl(link, child_set, final_set, max_results)\r\n",
        "    if not flag:\r\n",
        "      return False # Dừng khi đã đạt kết quả tối đa\r\n",
        "  return True # dừng sau khi duyệt hết"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNlD_GJ6JA69"
      },
      "source": [
        "DEPTH = int(input(\"Độ sâu mà bạn muốn crawl: \"))\r\n",
        "MAX_RESULTS = int(input(\"Số link mà bạn muốn lấy: \"))\r\n",
        "seed = input(\"url khởi đầu: \")\r\n",
        "final_set = set([seed])\r\n",
        "set_0 = set([seed])\r\n",
        "history = []\r\n",
        "url_list = []\r\n",
        "url_list.append(set_0)\r\n",
        "for x in range(DEPTH):\r\n",
        "  new_set = set()\r\n",
        "  url_list.append(new_set)\r\n",
        "for i in range(DEPTH):\r\n",
        "  flag = crawl_set(url_list[i], url_list[i+1], final_set, MAX_RESULTS)\r\n",
        "  if not flag: # Đã đạt được số kết quả tối đa hay chưa\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiDgrXijZQNg"
      },
      "source": [
        "for s in url_list:\r\n",
        "  print(len(s))\r\n",
        "print(len(final_set))\r\n",
        "for i in final_set:\r\n",
        "  append_to_file('crawled.txt',i)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}